{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Preprocess\n",
    "\n",
    "テキストや音声、画像をモデルに渡して推論・訓練を行うためには、事前にそれらをモデルが期待する形式に変換しておく必要があります。\n",
    "本チュートリアルでは、`Transformers` ライブラリが提供する、データの事前処理の手法について学びます。\n",
    "\n",
    "本チュートリアルは、[Hugging Face Transformers チュートリアル](https://huggingface.co/docs/transformers/v4.57.1/ja/preprocessing) を元に、一部加筆・修正して作成しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "このチュートリアルコードをすべて実行するためには、明示的に `import` するライブラリの他に、以下のソフトウェアが必要です。\n",
    "\n",
    "- [`tesseract`](https://github.com/tesseract-ocr/tesseract) (および、その Python ラッパー: `pytesseract`): 動画処理\n",
    "- `torch` ライブラリ or `tensorflow` ライブラリ: バックエンド\n",
    "    - 本チュートリアルでは `torch` を用いるコードしか紹介しません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you are working in google colab\n",
    "\n",
    "%pip install pytesseract torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import (\n",
    "    ColorJitter,\n",
    "    Compose,\n",
    "    RandomResizedCrop,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoImageProcessor,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "自然言語処理のタスクにおいて、テキストの事前処理に使用する主なアーキテクチャは**トーカナイザ**です。\n",
    "トーカナイザは、テキストを一定のルールのもとで**トークン**に分割します。\n",
    "トークンは単語や文字、あるいは単語を構成する部分文字列で構成されます。\n",
    "個々のトークンに識別番号を振ることでテキストが数列に変換され、これにより、機械学習モデルが文字列を数理的に取り扱えるようになります。\n",
    "\n",
    "ここでも、`from_pretrained()` メソッドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: google-bert/bert-base-cased (110M params)\n",
    "# ref: https://huggingface.co/google-bert/bert-base-cased\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "ここで、トーカナイザの出力について補足しておきます。\n",
    "\n",
    "- `input_ids`: 文中の各トークンに対応するインデックス\n",
    "- `token_type_ids`: 複数の文が入力された場合に、それらを区別するために付与される id の列\n",
    "- `attention_mask`: attention アーキテクチャがトークンを受け取る必要があるかを示す bool の列\n",
    "\n",
    "`input_ids` をデコードすることで元の入力が得られます。\n",
    "ここでわかるように、トーカナイザは文章に自動的に特別なトークン (`CLS`, `SEP`) を付与します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "複数の文章の前処理を行うこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs1 = tokenizer(batch_sentences)\n",
    "print(encoded_inputs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Padding\n",
    "\n",
    "テキストは常に同じ長さ (同じトークン数) とは限りませんが、推論モデルはある特定の長さの入力しか受け付けることができません。\n",
    "そこで、トーカナイザはテキストをトークン化しつつ、その長さを揃えることが期待されます。\n",
    "\n",
    "このための戦略の1つがパディングです。\n",
    "`padding=True` を指定することで、入力バッチ中の最長のテキストに合わせて、短いテキストに**パディングトークン**が追加されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs2 = tokenizer(batch_sentences, padding=True)\n",
    "print(encoded_inputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Truncation\n",
    "\n",
    "入力テキストの長さが、モデルが期待する入力次元を超えてしまう場合があります。\n",
    "`truncation=True` を指定することで、モデルが受け入れる最大の長さにトークン列を切り詰めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs3 = tokenizer(batch_sentences, padding=True, truncation=True)\n",
    "print(encoded_inputs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "ちなみに、今回利用している \"google-bert/bert-base-cased\" モデルが浮き入れる最大トークン数は 512 であるため、`batch_sentences` に含まれている入力テキスト程度のトークン数では Truncation は有効に効いてきません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "print(bert_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Build tensors\n",
    "\n",
    "`return_tensors=\"pt\"` (`\"tf\"`) を指定することで、出力を `PyTorch` (`TensorFlow`) のテンソル形式に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "encoded_inputs4 = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(encoded_inputs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Audio\n",
    "\n",
    "音声処理タスクにおいて、音声データの事前処理に使用する主なアーキテクチャは**特徴抽出器**です。特徴抽出器は生の音声データから特徴を抽出し、それらをテンソルに変換します。\n",
    "\n",
    "まず、入力データセットをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_audio = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "dataset_audio.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "`dataset_audio.features` によると、サンプリングレートは 8 kHz であるようです。\n",
    "今回は Wav2Vec2 モデルへの入力を想定しますが、このモデルはサンプリングレート 16 kHz のデータで事前学習されているので、`dataset_audio` を 16 kHz でリサンプルしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_audio2 = dataset_audio.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset_audio2.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pvdt",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "次に、特徴抽出器を用いて入力データを正規化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: facebook/wav2vec2-base (95M params)\n",
    "# ref: https://huggingface.co/facebook/wav2vec2-base\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input1 = [dataset_audio2[0][\"audio\"][\"array\"]]\n",
    "print(feature_extractor(audio_input1))\n",
    "audio_input2 = [dataset_audio2[1][\"audio\"][\"array\"]]\n",
    "print(feature_extractor(audio_input2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "テキストデータと同様に、データセットに含まれる音声データの長さがすべて等しいとは限らず (`shape` メンバを参照) 、また、モデルが期待する入力の長さには限りがあります。\n",
    "そこで、パディングとトランケーションを行います。\n",
    "特徴抽出器では、最大サンプル長を制御するために `max_length=<number>` を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        max_length=100000,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset_audio = preprocess_function(dataset_audio2[:5])\n",
    "print(processed_dataset_audio[\"input_values\"][0].shape)\n",
    "print(processed_dataset_audio[\"input_values\"][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Computer Vision\n",
    "\n",
    "画像処理タスクにおいて、画像データの事前処理に使用する主なアーキテクチャは**画像プロセッサ**です。\n",
    "画像プロセッサは元の画像データに対してリサイズ・正規化・チャネル\n",
    "補正などの処理を行い、それらをテンソルに変換します。\n",
    "\n",
    "例によって、データセットと画像プロセッサを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cv = load_dataset(\"food101\", split=\"train[:100]\")\n",
    "dataset_cv[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: google/vit-base-patch16-224 (86.6M params)\n",
    "# ref: https://huggingface.co/google/vit-base-patch16-224\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TRpd",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "続いて、データセットに含まれる画像データを、所定の方法で処理していきます。\n",
    "ここでは、`torchvision` の `transforms` モジュールを使用します。\n",
    "ここで行う処理は以下のとおりです。\n",
    "\n",
    "- `RandomResizedCrop`: モデルが期待する画像サイズ (`image_size`) に合わせて、画像をランダムに切り抜く。\n",
    "- `ColorJitter`: 画像の色調や明るさをランダムに変化させる。\n",
    "    - `brightness=0.5`: 明るさを $\\pm 50 \\%$ の範囲でランダムに変化させる。\n",
    "    - `hue=0.5`: 色相値を $\\pm 0.5$ の範囲でランダムに変化させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "transforms_core = Compose([\n",
    "    RandomResizedCrop(image_size),\n",
    "    ColorJitter(brightness=0.5, hue=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "`transforms_core` をデータセットの各画像に適用し、画像プロセッサで処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    images = [transforms_core(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cv.set_transform(transforms)\n",
    "dataset_cv[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset_cv[0][\"pixel_values\"]\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAgl",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Padding\n",
    "\n",
    "データセットに含まれる画像のサイズが異なる場合には、`DataImageProcessor.pad()` によってパディングを施します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dGlV",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Multi Modal\n",
    "\n",
    "マルチモーダルタスクにおいて、データの事前処理に使用する主なアーキテクチャは**プロセッサ**です。 プロセッサはトーカナイザや特徴抽出器などの複数の事前処理アーキテクチャを結合します。\n",
    "\n",
    "まずはデータセットを読み込みます。\n",
    "[オリジナルのチュートリアル](https://huggingface.co/docs/transformers/v4.57.1/ja/preprocessing) では `lj_speech` というデータセットが読み込まれていますが、このデータセットは最新の `datasets` (version 4.3.0) ではサポートされていないので、別のデータセットをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech1 = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", split=\"validation\")\n",
    "librispeech1.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgWD",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "今回興味があるのは `audio` と `text` だけなので、それ以外のメンバを削除してしまいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yOPj",
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech2 = librispeech1.map(remove_columns=[\"file\", \"id\", \"chapter_id\", \"speaker_id\"])\n",
    "print(librispeech2.features)\n",
    "print(librispeech2[0][\"audio\"][\"sampling_rate\"])\n",
    "print(librispeech2[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fwwy",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "`librispeech_asr_demo` データセットはサンプリングレートが 16 kHz でモデルの事前学習データセットのサンプリングレートと一致しているので、リサンプリングの必要はありません。\n",
    "安心してモデルを読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LJZf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: facebook/wav2vec2-base-960h (95M params)\n",
    "# ref: https://huggingface.co/facebook/wav2vec2-base-960h\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urSm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "`processor` に `audio` と `text` を指定して、事前処理を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxvo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example.update(processor(audio=audio[\"array\"], text=example[\"text\"], sampling_rate=16000))\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mWxS",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(librispeech2[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
