{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd7420c",
   "metadata": {},
   "source": [
    "# Pipelines for inferenece\n",
    "\n",
    "本チュートリアルでは、`pieline()` を用いて、訓練済みモデルによって様々な推論タスクを実行する手法を学びます。\\\n",
    "なお、ここで紹介されている多くのモデルは非常に沢山のパラメータをもつため、ロードのために多くのメモリやストレージを必要とします。\\\n",
    "ローカルで実行する際は注意してください。\n",
    "\n",
    "本チュートリアルは、[Hugging Face Transformers チュートリアル](https://huggingface.co/docs/transformers/v4.57.0/ja/pipeline_tutorial) を元に、一部加筆・修正して作成しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec0910",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "このチュートリアルコードをすべて実行するためには、明示的に `import` するライブラリの他に、以下のソフトウェアが必要です。\n",
    "\n",
    "- [`ffmpeg`](https://www.ffmpeg.org/) (および、その Python ラッパー: `ffmpeg-python`): 動画処理\n",
    "- [`tesseract`](https://github.com/tesseract-ocr/tesseract) (および、その Python ラッパー: `pytesseract`): 画像処理\n",
    "- `accelerate` ライブラリ: モデルの自動配置\n",
    "- `bitsandbytes` ライブラリ: モデルの量子化\n",
    "    - linux, windows のみサポート\n",
    "- `pillow` ライブラリ: 画像処理\n",
    "- `torchcodec` ライブラリ: 動画処理\n",
    "\n",
    "もし自分の環境にインストールされていない場合には、事前にインストールしておいてください。\\\n",
    "なお、`ffmpeg` と `tesseract` に関しては、macOSであれば [`Homebrew`](https://formulae.brew.sh) から簡単にインストールできるようです (動作未確認) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ef203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you are working in google colab\n",
    "\n",
    "!pip install accelerate bitsandbytes datasets ffmpeg-python transformers torch torchcodec pillow pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b093c6e",
   "metadata": {},
   "source": [
    "## Pipeline usage\n",
    "\n",
    "`pipeline(task=\"task\")` により、推論タスク `\"task\"` を行うデフォルトのモデルが提供されます。\\\n",
    "提供される構造体は、事前処理・推論・事後処理をワンライナーで行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare generator with task name\n",
    "# default model: \"facebook/wav2vec2-base-960h\" (94.4M params)\n",
    "# ref: (https://huggingface.co/facebook/wav2vec2-base-960h)\n",
    "\n",
    "generator = pipeline(task=\"automatic-speech-recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39734103",
   "metadata": {},
   "source": [
    "ファイルを指定して、推論を行います。\n",
    "\n",
    "<audio src=\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\" controls></audio>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate with input text\n",
    "# target: \"I have a dream that one day this nation will rise up and live out the true meaning of its creed ...\" (Martin Luther King Jr.)\n",
    "\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cc773",
   "metadata": {},
   "source": [
    "具体的なモデルを指定するには、`pipeline(model=\"model\")` とします。\\\n",
    "モデルの一覧は [`Hub`](https://huggingface.co/models) から確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare generator with model name\n",
    "# superior model: \"openai/whisper-large\" (1.54B params, heavy)\n",
    "\n",
    "generator = pipeline(model=\"openai/whisper-large\")\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94391fa",
   "metadata": {},
   "source": [
    "複数の入力を `list` で受け取ることもできます。\n",
    "\n",
    "<audio src=\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\" controls></audio>\n",
    "<audio src=\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\" controls></audio>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 1: \"I have a dream that one day this nation will rise up and live out the true meaning of its creed ...\" (Martin Luther King Jr.)\n",
    "# target 2: \"He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour-fattened sauce.\" (James Joyce, \"A Portrait Of The Artist As A Young Man\")\n",
    "\n",
    "generator(\n",
    "    [\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d70644",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "`pipeline()` はタスク固有・非固有の多くのパラメータをサポートしています。\\\n",
    "一般的には、このパラメータはどこでも指定できます。\n",
    "\n",
    "```python\n",
    "generator = pipeline(model=\"openai/whisper-large\", my_parameter=1)\n",
    "out = generator(...)                  # `my_parameter=1` is used here\n",
    "out = generator(..., my_parameter=2)  # `my_parameter=2` is used here (overwritten)\n",
    "out = generator(...)                  # `my_parameter=1` is used again here\n",
    "```\n",
    "\n",
    "以下で、特に重要なパラメータを確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c5e97",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "`device=n` を使用すると、モデルが指定したデバイスに自動的に配置されます。\\\n",
    "具体的な use case は以下の通りです。\n",
    "\n",
    "- `device=-1`: CPU\n",
    "- `device=n` (non-negative integer): id `n` に対応する GPU\n",
    "    - id はマシン上の各 GPU に自動的に割り振られています\n",
    "    - NVIDIA GPU であれば `torch.cuda.get_device_name(n)` で id `n` に対応する GPU の名前が取得できます\n",
    "\n",
    "なお、特に指定しない場合には、GPU を使用するように自動的にデバイスが決定されるようです。\\\n",
    "筆者の環境 (Macbook Air M4) では、Apple GPU (`mps`) が自動的に設定されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b15d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run \"openai/whisper-large\" on GPU[0]\n",
    "\n",
    "generator = pipeline(model=\"openai/whisper-large\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98feebf2",
   "metadata": {},
   "source": [
    "もしモデルが単一のGPUには大きすぎる場合、`device_map=\"auto\"` を指定して、`🤗 Accelerate` にモデルの重みをどのようにロードし、保存するかを自動的に決定させることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run \"openai/whisper-large\" on GPU (loaded)\n",
    "\n",
    "generator = pipeline(model=\"openai/whisper-large\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03e539",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "`batch_size=n` を指定することで、バッチサイズ `n` で推論することができます。\\\n",
    "ただし、バッチ処理は必ずしも速いわけではなく、実際にいくつかのケースでかなり遅くなることが確認されているようです。\\\n",
    "なお、バッチ処理を行ったとしても、得られる結果はバッチ処理を行わない場合と一致します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched inference\n",
    "\n",
    "generator = pipeline(model=\"openai/whisper-large\", device=0, batch_size=2)\n",
    "audio_filenames = [f\"audio_{i}.flac\" for i in range(10)]\n",
    "texts = generator(audio_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663de307",
   "metadata": {},
   "source": [
    "### Task specific parameters\n",
    "\n",
    "すべてのタスクはタスク固有のパラメータを提供しています。\\\n",
    "たとえば、`transformers.AutomaticSpeechRecognitionPipeline.call()` メソッドには、ビデオの字幕作成に有用な `return_timestamps` パラメータがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb60259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech recognition with time stamp\n",
    "# model: \"facebook/wav2vec2-large-960h-lv60-self\" (317M params)\n",
    "# ref: https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\n",
    "\n",
    "generator = pipeline(model=\"facebook/wav2vec2-large-960h-lv60-self\", return_timestamps=\"word\")\n",
    "generator(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a8713",
   "metadata": {},
   "source": [
    "## Using pipeline in a dataset\n",
    "\n",
    "`pipeline()` は大規模なデータセット上で推論を実行することもできます。\\\n",
    "例えば、イテレータを用いて、これを簡単に実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82163ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech recognition with time stamp\n",
    "# model: \"openai-community/gpt2\" (137M params)\n",
    "# ref: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "def data():\n",
    "    for i in range(1000):\n",
    "        yield f\"My example {i}\"\n",
    "\n",
    "pipe = pipeline(model=\"openai-community/gpt2\", device=0)\n",
    "generated_characters = 0\n",
    "for out in pipe(data()):\n",
    "    generated_characters += len(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ae8df",
   "metadata": {},
   "source": [
    "`🤗 Datasets` からデータセットをロードして繰り返し反復させることもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyDataset is a util that will just output the item we're interested in.\n",
    "\n",
    "pipe = pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n",
    "\n",
    "for out in pipe(KeyDataset(dataset, \"audio\")):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88370ab",
   "metadata": {},
   "source": [
    "## Using pipelines for a webserver\n",
    "\n",
    "skip this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6d9e4",
   "metadata": {},
   "source": [
    "## Vision pipeline\n",
    "\n",
    "画像処理タスクに `pipeline()` を用いる方法もほぼ同じです。\\\n",
    "ここでは、写真上のオブジェクトを分類する推論タスクを実行しています。\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: \"google/vit-base-patch16-224\" (86.6M params)\n",
    "# ref: https://huggingface.co/google/vit-base-patch16-224\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "preds = vision_classifier(\n",
    "    inputs=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42865868",
   "metadata": {},
   "source": [
    "## Text pipeline\n",
    "\n",
    "上と同様です。\\\n",
    "ここでは、テキストのコンテンツを分類する推論タスクを実行しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: \"facebook/bart-large-mnli\" (407M params)\n",
    "# ref: https://huggingface.co/facebook/bart-large-mnli\n",
    "\n",
    "classifier = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510654f",
   "metadata": {},
   "source": [
    "## Multimodal pipeline\n",
    "\n",
    "`pipeline()` は、複数のモダリティをサポートしています。\\\n",
    "たとえば、テキスト処理と画像処理を組み合わせて、画像についてテキストベースの推論タスクを実行することができます。\n",
    "\n",
    "<img src=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b12a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: \"impira/layoutlm-document-qa\" (128M params)\n",
    "# ref: https://huggingface.co/impira/layoutlm-document-qa\n",
    "\n",
    "vqa = pipeline(model=\"impira/layoutlm-document-qa\")\n",
    "output = vqa(\n",
    "    image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\",\n",
    "    question=\"What is the invoice number?\",\n",
    ")\n",
    "output[0][\"score\"] = round(output[0][\"score\"], 3)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907e3db",
   "metadata": {},
   "source": [
    "## Using pipeline on large models with 🤗 accelerate\n",
    "\n",
    "`device_map=\"auto\"` を指定して、大規模モデルを使用可能なデバイスに適切に分配してロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32efaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: \"facebook/opt-1.3b\" (1.3B params, heavy)\n",
    "# ref: https://huggingface.co/facebook/opt-1.3b\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe(\"これは素晴らしい例です！\", do_sample=True, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97031ae3",
   "metadata": {},
   "source": [
    "さらに、`bitsandbytes` をインストールし、`load_in_8bit=True` を指定すれば、モデルを 8 bit で読み込むことができます。\\\n",
    "なお、`bitsandbytes` は現状 linux と windows しかサポートしていません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63735f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n",
    "pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
