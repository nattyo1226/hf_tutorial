{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ae4248",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Load pretrained instances with an AutoClass\n",
    "\n",
    "本チュートリアルでは、`AutoClass.from_pretrained()` を用いて、訓練済みモデルから目的のアーキテクチャをロードする方法を学びます。\\\n",
    "`pipeline()` が事前処理・推論・事後処理を一挙に行うモデル全体を提供していたのとは対照的に、`AutoClass.from_pretrained()` はモデルを構成する各アーキテクチャを選択的にロードし、別のタスクのために利用することができます。\n",
    "\n",
    "本チュートリアルは、[Hugging Face Tranformers チュートリアル](https://huggingface.co/docs/transformers/v4.57.0/ja/autoclass_tutorial) を元に、一部加筆・修正して作成しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb81424",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "このチュートリアルコードをすべて実行するためには、明示的に `import` するライブラリの他に、以下のソフトウェアが必要です。\n",
    "\n",
    "- `torch` ライブラリ (or `tensorflow` ライブラリ): `AutoModelForSequenceClassification` クラスなどのバックエンド\n",
    "    - 本チュートリアルでは `torch` を用いるコードしか紹介しません\n",
    "\n",
    "もし自分の環境にインストールされていない場合には、事前にインストールしておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e89e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you are working in google colab\n",
    "\n",
    "!pip install librosa pillow torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f78e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harunama/project/hf_tutorial/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import librosa\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8429f",
   "metadata": {},
   "source": [
    "## AutoTokenizer\n",
    "\n",
    "Tokenizer は、入力テキストにトークン化・テンソル化などの変換を行うアーキテクチャです。\\\n",
    "ここでは `AutoTokenizer` クラスを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ba9749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# model: \"google-bert/bert-base-uncased\" (110M params)\n",
    "# ref: https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "sequence = \"In a hole in the ground there lived a hobbit.\"\n",
    "print(tokenizer(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86325e1c",
   "metadata": {},
   "source": [
    "## AutoImageProcessor\n",
    "\n",
    "画像プロセッサは、入力画像にサイズ変更・正規化・テンソル化などの変更を行うアーキテクチャです。\\\n",
    "ここでは `AutoImageProcessor` クラスを用います。\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4fe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': [array([[[-0.05882353, -0.10588235, -0.21568626, ...,  0.19215691,\n",
      "          0.1686275 ,  0.09803927],\n",
      "        [-0.02745098, -0.0745098 , -0.17647058, ...,  0.2313726 ,\n",
      "          0.20000005,  0.12941182],\n",
      "        [-0.01176471, -0.06666666, -0.17647058, ...,  0.26274514,\n",
      "          0.22352946,  0.16078436],\n",
      "        ...,\n",
      "        [ 0.79607844,  0.79607844,  0.8039216 , ..., -0.8666667 ,\n",
      "         -0.8666667 , -0.8352941 ],\n",
      "        [ 0.8039216 ,  0.8039216 ,  0.8039216 , ..., -0.7647059 ,\n",
      "         -0.8117647 , -0.8117647 ],\n",
      "        [ 0.8117647 ,  0.81960785,  0.8117647 , ..., -0.7176471 ,\n",
      "         -0.79607844, -0.8117647 ]],\n",
      "\n",
      "       [[-0.03529412, -0.08235294, -0.19215685, ...,  0.15294123,\n",
      "          0.12941182,  0.05882359],\n",
      "        [-0.00392157, -0.05098039, -0.15294117, ...,  0.19215691,\n",
      "          0.16078436,  0.09019613],\n",
      "        [ 0.01176476, -0.04313725, -0.15294117, ...,  0.22352946,\n",
      "          0.18431377,  0.12156868],\n",
      "        ...,\n",
      "        [ 0.84313726,  0.84313726,  0.8509804 , ..., -0.90588236,\n",
      "         -0.90588236, -0.8745098 ],\n",
      "        [ 0.8509804 ,  0.8509804 ,  0.8509804 , ..., -0.8039216 ,\n",
      "         -0.8509804 , -0.8509804 ],\n",
      "        [ 0.85882354,  0.8666667 ,  0.85882354, ..., -0.75686276,\n",
      "         -0.8352941 , -0.8509804 ]],\n",
      "\n",
      "       [[ 0.03529418, -0.01176471, -0.12156862, ...,  0.12156868,\n",
      "          0.09803927,  0.02745104],\n",
      "        [ 0.06666672,  0.0196079 , -0.08235294, ...,  0.16078436,\n",
      "          0.12941182,  0.05882359],\n",
      "        [ 0.082353  ,  0.02745104, -0.08235294, ...,  0.19215691,\n",
      "          0.15294123,  0.09019613],\n",
      "        ...,\n",
      "        [ 0.9372549 ,  0.9372549 ,  0.94509804, ..., -0.92941177,\n",
      "         -0.92941177, -0.8980392 ],\n",
      "        [ 0.94509804,  0.94509804,  0.94509804, ..., -0.827451  ,\n",
      "         -0.8745098 , -0.8745098 ],\n",
      "        [ 0.9529412 ,  0.9607843 ,  0.9529412 , ..., -0.78039217,\n",
      "         -0.85882354, -0.8745098 ]]], shape=(3, 224, 224), dtype=float32)]}\n"
     ]
    }
   ],
   "source": [
    "# model: \"google/vit-base-patch16-224\" (86.6M params)\n",
    "# ref: https://huggingface.co/google/vit-base-patch16-224\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(io.BytesIO(requests.get(image_url).content))\n",
    "print(image_processor(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9bf36",
   "metadata": {},
   "source": [
    "## AutoFeatureExtractor\n",
    "\n",
    "特徴抽出器は、入力の画像や動画から一定の方法で特徴を抽出し、正規化・テンソル化などの変換を行うアーキテクチャです。\\\n",
    "ここでは、`AutoFeatureExtractor` クラスを用います。\n",
    "\n",
    "<audio src=\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\" controls></audio>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b977c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `Wav2Vec2FeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': [array([ 0.07173638,  0.04628736, -0.05077884, ..., -0.00460555,\n",
      "        0.03257194, -0.16154806], shape=(208000,), dtype=float32)], 'attention_mask': [array([1, 1, 1, ..., 1, 1, 1], shape=(208000,), dtype=int32)]}\n"
     ]
    }
   ],
   "source": [
    "# model: \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\" (316M params)\n",
    "# ref: https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\",\n",
    ")\n",
    "target_sr = feature_extractor.sampling_rate\n",
    "speech_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    "speech_bytes = io.BytesIO(requests.get(speech_url).content)\n",
    "speech, sr = librosa.load(speech_bytes, sr=None)\n",
    "if sr != target_sr:\n",
    "    speech = librosa.resample(speech, orig_sr=sr, target_sr=target_sr)\n",
    "    sr = target_sr\n",
    "print(feature_extractor(speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd224850",
   "metadata": {},
   "source": [
    "## AutoProcessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040c995",
   "metadata": {},
   "source": [
    "`Transformers` において、プロセッサはマルチモーダルモデルの入力に対して前処理を行うアーキテクチャを指します。\\\n",
    "ここでは、`AutoProcessor` クラスを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1999,  6767,  6610,   102,  1999,  6767,  6610,  2264,  7192,\n",
      "          4297,  1012,  4878, 11203,  4644,  2047,  2259,  1010,  6396, 13092,\n",
      "         10790,  3021,  3406,  2911,  3406,  1999,  6767,  6610,  1001,  2149,\n",
      "          1011, 25604,  2198,  3044,  2198,  3044,  1999,  6767,  6610,  3058,\n",
      "         11118,  2692, 17465, 11387, 16147,  1016,  2457,  2675,  1024,  4261,\n",
      "          2620,  2581,  7222,  8584,  3298, 13433,  2015,  2047,  2259,  1010,\n",
      "          6396, 13092, 10790,  4729,  1010,  5003, 13092, 10790,  1037, 20304,\n",
      "          2475,  1013, 10476,  2349,  3058, 24441,  2692, 17465, 11387, 16147,\n",
      "          1037,  2100,  6412,  3131,  3976,  3815,  1015,  2392,  5685,  4373,\n",
      "         13428, 15196,  2531,  1012,  4002,  2531,  1012,  4002,  1016,  2739,\n",
      "         18903,  2546, 15749,  2572,  5244,  2321,  1012,  4002,  2382,  1012,\n",
      "          4002,  1017,  4450, 16425,  2869,  3156, 10347,  4942,  3406,  9080,\n",
      "         13741,  1012,  4002, 16183,  2891,  4171,  1020,  1012,  2423,  1003,\n",
      "         18744,  2561,  1002, 16666,  1012,  5757,  1055,  3408,  1004,  3785,\n",
      "          7909,  2483,  2349,  2306,  2321,  2420,  1022,  1049,  1059,  1052,\n",
      "          5003,  5705, 15775,  6169,  1055,  1051,  1024,  9765, 20996,  4502,\n",
      "           102],\n",
      "        [  101,  2193,   102,  1999,  6767,  6610,  2264,  7192,  4297,  1012,\n",
      "          4878, 11203,  4644,  2047,  2259,  1010,  6396, 13092, 10790,  3021,\n",
      "          3406,  2911,  3406,  1999,  6767,  6610,  1001,  2149,  1011, 25604,\n",
      "          2198,  3044,  2198,  3044,  1999,  6767,  6610,  3058, 11118,  2692,\n",
      "         17465, 11387, 16147,  1016,  2457,  2675,  1024,  4261,  2620,  2581,\n",
      "          7222,  8584,  3298, 13433,  2015,  2047,  2259,  1010,  6396, 13092,\n",
      "         10790,  4729,  1010,  5003, 13092, 10790,  1037, 20304,  2475,  1013,\n",
      "         10476,  2349,  3058, 24441,  2692, 17465, 11387, 16147,  1037,  2100,\n",
      "          6412,  3131,  3976,  3815,  1015,  2392,  5685,  4373, 13428, 15196,\n",
      "          2531,  1012,  4002,  2531,  1012,  4002,  1016,  2739, 18903,  2546,\n",
      "         15749,  2572,  5244,  2321,  1012,  4002,  2382,  1012,  4002,  1017,\n",
      "          4450, 16425,  2869,  3156, 10347,  4942,  3406,  9080, 13741,  1012,\n",
      "          4002, 16183,  2891,  4171,  1020,  1012,  2423,  1003, 18744,  2561,\n",
      "          1002, 16666,  1012,  5757,  1055,  3408,  1004,  3785,  7909,  2483,\n",
      "          2349,  2306,  2321,  2420,  1022,  1049,  1059,  1052,  5003,  5705,\n",
      "         15775,  6169,  1055,  1051,  1024,  9765, 20996,  4502,   102,     0,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'bbox': tensor([[[   0,    0,    0,    0],\n",
      "         [   0,    0,    0,    0],\n",
      "         [   0,    0,    0,    0],\n",
      "         ...,\n",
      "         [ 804,  937,  853,  949],\n",
      "         [ 804,  937,  853,  949],\n",
      "         [1000, 1000, 1000, 1000]],\n",
      "\n",
      "        [[   0,    0,    0,    0],\n",
      "         [   0,    0,    0,    0],\n",
      "         [1000, 1000, 1000, 1000],\n",
      "         ...,\n",
      "         [1000, 1000, 1000, 1000],\n",
      "         [   0,    0,    0,    0],\n",
      "         [   0,    0,    0,    0]]]), 'image': tensor([[[[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]]],\n",
      "\n",
      "\n",
      "        [[[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]]]], dtype=torch.uint8)}\n"
     ]
    }
   ],
   "source": [
    "# model: \"microsoft/layoutlmv2-base-uncased\" (200M params)\n",
    "# ref: https://huggingface.co/microsoft/layoutlmv2-base-uncased\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "image_url = \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n",
    "image = Image.open(io.BytesIO(requests.get(image_url).content)).convert(\"RGB\")\n",
    "text = [\"invoice\", \"number\"]\n",
    "print(processor(images=[image, image], text=text, return_tensors=\"pt\", padding=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c3ba9",
   "metadata": {},
   "source": [
    "## AutoModel\n",
    "\n",
    "特定のタスクに対して訓練済みモデルを `torch.Tensor` (`tf.Tensor`) 形式でロードするためには、`AutoModelFor` クラス (`TFAutoModelFor` クラス) を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bc2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model: \"distilbert/distilbert-base-uncased\" (67M params)\n",
    "# ref: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "\n",
    "AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932571dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
