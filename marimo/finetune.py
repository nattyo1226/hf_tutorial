import marimo

__generated_with = "0.17.2"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # Fine-tune a pretrained model

    æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚
    äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¿½åŠ ã®å­¦ç¿’ã‚’è¡Œå¾—ã“ã¨ã§ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾—ã‚‹ã“ã¨ãŒæœŸå¾…ã§ãã¾ã™ã€‚

    æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ã€[Hugging Face Tranformers ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/transformers/v4.57.1/ja/training) ã‚’å…ƒã«ã€ä¸€éƒ¨åŠ ç­†ãƒ»ä¿®æ­£ã—ã¦ä½œæˆã—ã¦ã„ã¾ã™ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Dependencies

    ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ã™ã¹ã¦å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã¯ã€æ˜ç¤ºçš„ã« `import` ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä»–ã«å¿…è¦ãªã‚‚ã®ã¯ç‰¹ã«ã‚ã‚Šã¾ã›ã‚“ã€‚
    """
    )
    return


@app.cell
def _():
    # run this cell if you are working in google colab

    # %pip install evaluate
    return


@app.cell
def _():
    from datasets import load_dataset
    import evaluate
    import numpy as np
    from transformers import (
        AutoModelForSequenceClassification,
        AutoTokenizer,
        get_scheduler,
        Trainer,
        TrainingArguments,
    )
    import torch
    from torch.optim import AdamW
    from torch.utils.data import DataLoader
    from tqdm.auto import tqdm
    return (
        AutoModelForSequenceClassification,
        AutoTokenizer,
        Trainer,
        TrainingArguments,
        evaluate,
        load_dataset,
        np,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Prepare a dataset

    ä»Šå›ã¯ã€[google-bert/bert-base-cased](https://huggingface.co/google-bert/bert-base-cased) (BERT) ã¨ã„ã† Masked Language Model (MLM) ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã—ã¦ã¿ã¾ã™ã€‚
    MLM ã¨ã¯ã€æ–‡ç« ä¸­ã®éš ã•ã‚ŒãŸ (masked) éƒ¨åˆ†ã«å½“ã¦ã¯ã¾ã‚‹å˜èªã‚’æ¨æ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’è¡Œã†è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
    BERT ã¯ [BookCorpus](https://yknzhu.wixsite.com/mbweb) ã¨å‘¼ã°ã‚Œã‚‹ã€11308 å†Šã®æœªå‡ºç‰ˆæ›¸ç±ã¨ã€è‹±èªã® Wikipedia ã«ã‚ˆã£ã¦äº‹å‰å­¦ç¿’ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚

    ä»Šå›ã¯ã€BERT ã«å¯¾ã—ã¦ã€[yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full) ã¨ã„ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚
    ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€é£²é£Ÿåº—ã‚„åº—èˆ—ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚µã‚¤ãƒˆã§ã‚ã‚‹ Yelp ä¸Šã®ãƒ¬ãƒ“ãƒ¥ãƒ¼æ–‡ç« ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
    æ—©é€Ÿã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†ã€‚
    """
    )
    return


@app.cell
def _(load_dataset):
    dataset = load_dataset("yelp_review_full")
    dataset["train"][100]
    return (dataset,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒãƒ­ãƒ¼ãƒ‰ã§ããŸã‚‰ã€ãƒˆãƒ¼ã‚«ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦äº‹å‰å‡¦ç†ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚""")
    return


@app.cell
def _(AutoTokenizer, dataset):
    tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True)

    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    return (tokenized_datasets,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""å®Ÿè¡Œæ™‚é–“çŸ­ç¸®ã®ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é©å½“ãªéƒ¨åˆ†ã‚»ãƒƒãƒˆã‚’ä½œæˆã§ãã¾ã™ã€‚""")
    return


@app.cell
def _(tokenized_datasets):
    small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
    small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
    return small_eval_dataset, small_train_dataset


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Train

    è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ãŒã§ããŸã®ã§ã€ã“ã“ã‹ã‚‰æœ¬æ ¼çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ã¦ã„ãã¾ã™ã€‚
    ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†æ‰‹æ³•ã«ã¯ã„ãã¤ã‹ã‚ã‚Šã€ä»£è¡¨çš„ãªã‚‚ã®ã¯ä»¥ä¸‹ã® 3 é€šã‚Šã§ã™ã€‚

    - `ğŸ¤— Transformers` ãŒæä¾›ã™ã‚‹ `Trainer` ã‚¯ãƒ©ã‚¹ã‚’åˆ©ç”¨ã™ã‚‹ã‚‚ã®
    - `Kelas` API ã‚’åˆ©ç”¨ã—ã¦ `TensorFlow` ã§è¨“ç·´ã™ã‚‹ã‚‚ã®
    - ãƒã‚¤ãƒ†ã‚£ãƒ–ã® `PyTorch` ã§è¨“ç·´ã™ã‚‹ã‚‚ã®

    ã“ã‚Œã‚‰ã®åˆ©ç‚¹ãƒ»æ¬ ç‚¹ã¯ä¸‹è¡¨ã®ã¨ãŠã‚Šã§ã™ (`GPT-5` ãŠã‚ˆã³ `gemini-2.5-pro` ã«ã‚ˆã‚‹å›ç­”ã®ã¾ã¨ã‚) ã€‚

    | method | advantage ğŸ‘ | disadvantage ğŸ‘ |
    | :---- | :---- | :---- |
    | `Trainer` | é«˜æ°´æº– API ã‚’ç”¨ã„ã¦çŸ­ã„ã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã§ãã‚‹ | æŸ”è»Ÿæ€§ã«æ¬ ã‘ã‚‹ |
    | `Kelas` + `TensorFlow` | `TensorFlow` ã«æ…£ã‚Œã¦ã„ã‚‹äººã«ã¯è¦ªã—ã¿ã‚„ã™ã„ã€`Trainer` ã‚ˆã‚Šã¯æŸ”è»Ÿã«å®Ÿè£…ã§ãã‚‹ | `ğŸ¤— Transformers` ã¯ãã‚‚ãã‚‚ `PyTorch` ä¸­å¿ƒã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®è¦æ¨¡ãŒ `PyTorch` ã«æ¯”ã¹ã¦å°ã•ã„ |
    | Native `PyTorch` | ä½æ°´æº– API ã‚’ç”¨ã„ã¦æŸ”è»Ÿã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã‚‹ | ã‚³ãƒ¼ãƒ‰é‡ãŒå¤šãã€å®Ÿè£…ãŒæ¯”è¼ƒçš„è¤‡é›‘ |

    æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€`Trainer` ã¨ Native `PyTorch` ã® 2 é€šã‚Šã®æ‰‹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Train with PyTorch Trainer

    `Trainer` ã‚¯ãƒ©ã‚¹ã‚’ç”¨ã„ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹é †ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
    `ğŸ¤— Transformers` ãŒæä¾›ã™ã‚‹é«˜æ°´æº– API ã‚’ç”¨ã„ã¦ã€æ•°è¡Œã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ç°¡æ½”ã«è¨˜è¿°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    ã¾ãšãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€äºˆæƒ³ã•ã‚Œã‚‹ (ãƒã‚¹ã‚¯ã•ã‚Œã‚‹) ãƒ©ãƒ™ãƒ«ã®æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _(AutoModelForSequenceClassification):
    model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
    return (model,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### Training Hyperparameters

    å­¦ç¿’æ™‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ `TrainingArguments` ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚
    å­¦ç¿’æ™‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã¯ã€ä¾‹ãˆã°å­¦ç¿’å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚„ã€æå¤±é–¢æ•°ã®å€¤ã®ãƒ­ã‚°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãªã©ã‚’å«ã¿ã¾ã™ã€‚
    ã¾ãŸã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã¯ã€å­¦ç¿’ç‡ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚„ã‚¨ãƒãƒƒã‚¯æ•°ãªã©ã‚’å«ã¿ã¾ã™ã€‚
    ä½•ã‚‚æŒ‡å®šã—ãªã‘ã‚Œã°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å€¤ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚

    ```python
    training_args = TrainingArguments(output_dir="test_trainer")
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### Evaluate

    `Trainer` ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã¾ã›ã‚“ã€‚
    ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚’è¡Œã†ã«ã¯ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¦å ±å‘Šã™ã‚‹é–¢æ•°ã‚’ `Trainer` ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    `ğŸ¤— Evaluate` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã¯ã€`evaluate.load` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã€`accuracy` é–¢æ•°ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _(evaluate):
    metric = evaluate.load("accuracy")
    return (metric,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    `metric.compute()` ã‚’å‘¼ã¶ã“ã¨ã§ã€äºˆæ¸¬ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    ãªãŠã€ã™ã¹ã¦ã® `ğŸ¤— Transformers` ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯ logit ã ãã†ã§ã™ã€‚
    """
    )
    return


@app.cell
def _(metric, np):
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)
    return (compute_metrics,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è¨ˆç®—ã—ãŸã„å ´åˆã€å­¦ç¿’å¼•æ•° `eval_strategy` ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚
    ä»Šå›ã¯ã€å„ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«è¨ˆç®—ã™ã‚‹ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _(TrainingArguments):
    training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")
    return (training_args,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### Trainer

    ãƒ¢ãƒ‡ãƒ«ã€å­¦ç¿’å¼•æ•°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŒ‡å®šã—ã¦ã€`Trainer` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _(
    Trainer,
    compute_metrics,
    model,
    small_eval_dataset,
    small_train_dataset,
    training_args,
):
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=small_train_dataset,
        eval_dataset=small_eval_dataset,
        compute_metrics=compute_metrics,
    )
    return (trainer,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""`Trainer.train()` ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒè¡Œã‚ã‚Œã¾ã™ã€‚""")
    return


@app.cell
def _(trainer):
    trainer.train()
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Train in native Pytorch

    æ¬¡ã«ã€ãƒã‚¤ãƒ†ã‚£ãƒ– `PyTorch` ã§ãƒ•ã‚¡ã‚¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†æ‰‹é †ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
    è¤‡é›‘ã§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ€§ã®é«˜ã„å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’æ§‹æˆã§ãã¾ã™ã€‚
    ãªãŠã€ä»¥ä¸‹ã®éƒ¨åˆ†ã¯ä¸Šã§å®šç¾©ã—ãŸå¤‰æ•°ã¨è¡çªã™ã‚‹ãŸã‚ã€`marimo` ã§ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹æ–¹ã¯ã€ä¸€åº¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ‡ã£ã¦ã€`Trainer` API ã«ã‚ˆã‚‹å®Ÿè£…éƒ¨åˆ†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã‹ã‚‰ã€ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

    ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã‚’è¡Œã†ã®ã§ã™ãŒã€

    1. ãƒ¢ãƒ‡ãƒ«ã¯ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚‰ãªã„ã®ã§ã€`text` åˆ—ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    2. ãƒ¢ãƒ‡ãƒ«ã¯å¼•æ•°ã®åå‰ã‚’ `labels` ã¨æœŸå¾…ã—ã¦ã„ã‚‹ã®ã§ã€`label` åˆ—ã‚’ `labels` ã«åå‰ã‚’å¤‰æ›´ã—ã¦ã„ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _():
    # run this cell if you are working on ipynb (google colab)

    # del model
    # del trainer
    # torch.cuda.empty_cache()
    return


@app.cell
def _():
    # tokenized_datasets_in_need = tokenized_datasets.remove_columns(["text"])
    # tokenized_datasets_pt = tokenized_datasets_in_need.rename_column("label", "labels")
    # tokenized_datasets_pt.set_format("torch")

    # small_train_dataset_pt = tokenized_datasets_pt["train"].shuffle(seed=42).select(range(1000))
    # small_eval_dataset_pt = tokenized_datasets_pt["test"].shuffle(seed=42).select(range(1000))
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### DataLoader 

    ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã® `DataLoader` ã‚’ä½œæˆã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦å–ã‚Šå‡ºã›ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _():
    # train_dataloader = DataLoader(small_train_dataset_pt, shuffle=True, batch_size=8)
    # eval_dataloader = DataLoader(small_eval_dataset_pt, batch_size=8)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ä½µã›ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚‚è¡Œã£ã¦ã—ã¾ã„ã¾ã™ã€‚
    ã‚„ã‚Šæ–¹ã¯ `Trainer` ã®å ´åˆã¨åŒã˜ã§ã™ã€‚
    """
    )
    return


@app.cell
def _():
    # model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### Opeimizer and learning rate scheduler

    ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½œæˆã—ã¾ã™ã€‚
    ã“ã“ã§ã¯ã€`AdamW` ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã‚’è¡Œã†ã“ã¨ã«ã—ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _():
    # optimizer = AdamW(model.parameters(), lr=5e-5)
    return


@app.cell
def _():
    # num_epochs = 3
    # num_training_steps = num_epochs * len(train_dataloader)
    # lr_scheduler = get_scheduler(
    #     name="linear",
    #     optimizer=optimizer,
    #     num_warmup_steps=0,
    #     num_training_steps=num_training_steps,
    # )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""ã¾ãŸã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ãƒ‡ãƒã‚¤ã‚¹ã‚’æŒ‡å®šã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚""")
    return


@app.cell
def _():
    # for NVIDIA GPU (CUDA)
    # device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    # for Apple GPU (MPS)
    # device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")

    # model.to(device)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### Train

    å­¦ç¿’ã®é€²æ—ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ã€`tqdm` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦é€²è¡ŒçŠ¶æ³ãƒãƒ¼ã‚’è¡¨ç¤ºã•ã›ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _():
    # progress_bar = tqdm(range(num_training_steps))

    # model.train()
    # for epoch in range(num_epochs):
    #     for batch_t in train_dataloader:
    #         batch_train = {k: v.to(device) for k, v in batch_t.items()}
    #         outputs_train = model(**batch_train)
    #         loss = outputs.loss
    #         loss.backward()

    #         optimizer.step()
    #         lr_scheduler.step()
    #         optimizer.zero_grad()
    #         progress_bar.update(1)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    #### Evaluate

    `Trainer` ã®éš›ã¨åŒæ§˜ã«ã€è©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’å°å…¥ã—ã¾ã™ã€‚
    ã“ã“ã§ã¯ã€å„ã‚¨ãƒãƒƒã‚¯ã®æœ€å¾Œã«ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è¨ˆç®—ã™ã‚‹ä»£ã‚ã‚Šã«ã€`add_batch` ã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®ãƒãƒƒãƒã‚’è“„ç©ã—ã¦ãŠãã€æœ€å¾Œã«ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚
    """
    )
    return


@app.cell
def _():
    # metric = evaluate.load("accuracy")
    # model.eval()
    # for batch_e in eval_dataloader:
    #     batch_eval = {k: v.to(device) for k, v in batch_e.items()}
    #     with torch.no_grad():
    #         outputs_eval = model(**batch_eval)

    #     logits = outputs_eval.logits
    #     predictions = torch.argmax(logits, dim=-1)
    #     metric.add_batch(predictions=predictions, references=batch["labels"])

    # metric.compute()
    return


@app.cell
def _():
    import marimo as mo
    return (mo,)


if __name__ == "__main__":
    app.run()
