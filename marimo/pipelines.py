import marimo

__generated_with = "0.17.7"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    # Pipelines for inference

    æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€`pipeline()` ã‚’ç”¨ã„ã¦ã€è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æ§˜ã€…ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚
    ã“ã“ã§ç´¹ä»‹ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã†ã¡ã„ãã¤ã‹ã¯éå¸¸ã«å¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚‚ã¤ãŸã‚ã€ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã«å¤šãã®ãƒ¡ãƒ¢ãƒªã‚„ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚
    ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹éš›ã¯æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

    æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ã€[Hugging Face Transformers ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://huggingface.co/docs/transformers/v4.57.1/ja/pipeline_tutorial) ã‚’å…ƒã«ã€ä¸€éƒ¨åŠ ç­†ãƒ»ä¿®æ­£ã—ã¦ä½œæˆã—ã¦ã„ã¾ã™ã€‚
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Dependencies

    ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ã™ã¹ã¦å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã¯ã€æ˜ç¤ºçš„ã« `import` ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä»–ã«ã€ä»¥ä¸‹ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãŒå¿…è¦ã§ã™ã€‚

    - [`ffmpeg`](https://www.ffmpeg.org/): éŸ³å£°å‡¦ç†
    - [`tesseract`](https://github.com/tesseract-ocr/tesseract) (ãŠã‚ˆã³ã€ãã® Python ãƒ©ãƒƒãƒ‘ãƒ¼: `pytesseract`): ç”»åƒå‡¦ç† (ocr)
    - `accelerate` ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•é…ç½®
    - `bitsandbytes` ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: ãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–
        - linux, windows ã®ã¿ã‚µãƒãƒ¼ãƒˆ
    - `pillow` ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: ç”»åƒå‡¦ç†
    - `torchcodec` ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: éŸ³å£°å‡¦ç†

    ã‚‚ã—è‡ªåˆ†ã®ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã«ã¯ã€äº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠã„ã¦ãã ã•ã„ã€‚\
    ãªãŠã€`ffmpeg` ã¨ `tesseract` ã«é–¢ã—ã¦ã¯ã€macOSã§ã‚ã‚Œã° [`Homebrew`](https://formulae.brew.sh) ã‹ã‚‰ç°¡å˜ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹ã‚ˆã†ã§ã™ (å‹•ä½œæœªç¢ºèª) ã€‚
    """)
    return


@app.cell
def _():
    # run this cell if you are working in google colab

    # %pip install bitsandbytes torchcodec pytesseract
    return


@app.cell
def _():
    # import dependencies

    from datasets import load_dataset
    import torch
    from transformers import pipeline
    from transformers.pipelines.pt_utils import KeyDataset
    return KeyDataset, load_dataset, pipeline, torch


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Pipeline usage

    `pipeline(task="task")` ã«ã‚ˆã‚Šã€æ¨è«–ã‚¿ã‚¹ã‚¯ `"task"` ã‚’è¡Œã†ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«ãŒæä¾›ã•ã‚Œã¾ã™ã€‚
    æä¾›ã•ã‚Œã‚‹æ§‹é€ ä½“ã¯ã€å…¥åŠ›ã«å¯¾ã—ã¦äº‹å‰å‡¦ç†ãƒ»æ¨è«–ãƒ»äº‹å¾Œå‡¦ç†ã‚’ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã§å®Ÿè¡Œã—ã¾ã™ã€‚

    <audio src="https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac" controls></audio>
    """)
    return


@app.cell
def _(pipeline):
    # default model: "facebook/wav2vec2-base-960h" (94.4M params)
    # ref: (https://huggingface.co/facebook/wav2vec2-base-960h)

    pipe_asr1 = pipeline(task="automatic-speech-recognition")
    pipe_asr1("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
    return (pipe_asr1,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    å…·ä½“çš„ãªæ¨è«–ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã«ã¯ã€`pipeline(model="model")` ã¨ã—ã¾ã™ã€‚
    ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã¯ [`Hub`](https://huggingface.co/models) ã‹ã‚‰ç¢ºèªã§ãã¾ã™ã€‚
    """)
    return


@app.cell
def _(pipeline):
    # prepare generator with model name
    # superior model: "openai/whisper-large" (1.54B params)
    # this may be too heavy for cpus ...

    pipe_asr2 = pipeline(model="openai/whisper-large")
    pipe_asr2("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    è¤‡æ•°ã®å…¥åŠ›ã‚’ `list` ã§å—ã‘å–ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

    <audio src="https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac" controls></audio>

    <audio src="https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac" controls></audio>
    """)
    return


@app.cell
def _(pipe_asr1):
    pipe_asr1([
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ])
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Parameter

    `pipeline()` ã¯ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒ»éå›ºæœ‰ã®å¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
    ä¸€èˆ¬çš„ã«ã¯ã€ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã©ã“ã§ã‚‚æŒ‡å®šã§ãã¾ã™ã€‚

    ```python
    pipe = pipeline(..., my_parameter=1)
    out = pipe(...)                  # `my_parameter=1` is used here
    out = pipe(..., my_parameter=2)  # `my_parameter=2` is used here
    out = pipe(...)                  # `my_parameter=1` is used here
    ```

    ä»¥ä¸‹ã§ã€ç‰¹ã«ã‚ˆãç”¨ã„ã‚‰ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Device

    `device=n` ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã—ãŸãƒ‡ãƒã‚¤ã‚¹ã®ãƒ¡ãƒ¢ãƒªã«é…ç½®ã•ã‚Œã¾ã™ã€‚
    å…·ä½“çš„ãª use case ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

    - `device=-1`: CPU
    - `device=n` (non-negative integer): on GPU with id `n`
        - id ã¯ãƒã‚·ãƒ³ã‚’æ§‹æˆã™ã‚‹å„ GPU ã«è‡ªå‹•çš„ã«å‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ã„ã¾ã™
        - NVIDIA GPU ã§ã‚ã‚Œã° `torch.cuda.get_device_name(n)` ã§ id `n` ã«å¯¾å¿œã™ã‚‹ GPU ãƒ‡ãƒã‚¤ã‚¹åãŒå–å¾—ã§ãã¾ã™

    ãªãŠã€ç‰¹ã« `device` ã®å€¤ã‚’æŒ‡å®šã—ãªãã¦ã‚‚ã€GPU ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«è‡ªå‹•çš„ã«ãƒ‡ãƒã‚¤ã‚¹ãŒæ±ºå®šã•ã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚
    ç­†è€…ã®ç’°å¢ƒ (M4 MacBook Air) ã§ã¯ã€Apple GPU (mps) ãŒè‡ªå‹•çš„ã«é¸æŠã•ã‚Œã¾ã—ãŸã€‚
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Batch Size

    `batch_size=n` ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ãƒãƒƒãƒã‚µã‚¤ã‚º `n` ã§æ¨è«–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    ãŸã ã—ã€ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã£ã¦å®Ÿè¡Œé€Ÿåº¦ã®å‘ä¸ŠãŒå¿…ãšã—ã‚‚æœŸå¾…ã§ãã‚‹ã‚ã‘ã§ã¯ãªãã€ã„ãã¤ã‹ã®ã‚±ãƒ¼ã‚¹ã§ã¯ã‹ãªã‚Šé…ããªã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚
    ãªãŠã€ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã£ãŸã¨ã—ã¦ã‚‚ã€å¾—ã‚‰ã‚Œã‚‹çµæœã¯ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã‚ãªã„å ´åˆã¨ä¸€è‡´ã—ã¾ã™ã€‚
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Task specific parameters

    ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚
    ä¾‹ãˆã°ã€`transformers.AutomaticSpeechRecognitionPipeline.call()` ãƒ¡ã‚½ãƒƒãƒ‰ã«ã¯ã€é©å½“ãªå˜ä½ã§æ¨è«–çµæœã‚’åŒºåˆ‡ã£ã¦ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨åŒæ™‚ã«å‡ºåŠ›ã™ã‚‹ `return_timestamps` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚
    """)
    return


@app.cell
def _(pipeline):
    # model: "facebook/wav2vec2-large-960h-lv60-self" (317M params)
    # ref: https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self

    pipe_asr3 = pipeline(model="facebook/wav2vec2-large-960h-lv60-self", return_timestamps="word")
    pipe_asr3("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Using pipeline in a dataset

    `pipeline()` ã¯å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã§æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
    """)
    return


@app.cell
def _(pipeline):
    # model: "openai-community/gpt2" (137M params)
    # ref: https://huggingface.co/openai-community/gpt2

    def data():
        for i in range(10):
            yield f"My example {i}"

    pipe_tg1 = pipeline(model="openai-community/gpt2", device=0)
    for out_tg1 in pipe_tg1(data()):
        print(out_tg1[0]["generated_text"][:250], "...")
        print("---\n")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    `ğŸ¤— Datasets` ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç¹°ã‚Šè¿”ã—åå¾©ã•ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
    """)
    return


@app.cell
def _(KeyDataset, load_dataset, pipeline):
    pipe_asr4 = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
    dataset_asr1 = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

    for out_asr1 in pipe_asr4(KeyDataset(dataset_asr1, "audio")):
        print(out_asr1)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Using pipelines for a webserver

    ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯é£›ã°ã—ã¾ã™ã€‚
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Vision pipeline

    ç”»åƒå‡¦ç†ã‚¿ã‚¹ã‚¯ã§ã®ä½¿ç”¨ä¾‹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚
    ã“ã“ã§ã¯ã€å†™çœŸã«å†™ã£ã¦ã„ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ†é¡ã™ã‚‹æ¨è«–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚

    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg" width="30%">
    """)
    return


@app.cell
def _(pipeline):
    # model: "google/vit-base-patch16-224" (86.6M params)
    # ref: https://huggingface.co/google/vit-base-patch16-224

    pipe_vc1 = pipeline(model="google/vit-base-patch16-224")
    preds_vc1 = pipe_vc1(
        inputs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg",
    )
    preds_vc1 = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds_vc1]
    preds_vc1
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Text pipeline

    ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¿ã‚¹ã‚¯ã§ã®ä½¿ç”¨ä¾‹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚
    ã“ã“ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ€§æ ¼ã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚
    """)
    return


@app.cell
def _(pipeline):
    # model: "facebook/bart-large-mnli" (407M params)
    # ref: https://huggingface.co/facebook/bart-large-mnli

    pipe_tc1 = pipeline(model="facebook/bart-large-mnli")
    pipe_tc1(
        "I have a problem with my iphone that needs to be resolved asap!!",
        candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Multimodal pipeline

    `pipeline()` ã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
    ã“ã“ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã¨ç”»åƒå‡¦ç†ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ç”»åƒã‹ã‚‰ã‚¤ãƒ³ãƒœã‚¤ã‚¹ç•ªå·ã‚’æ¨è«–ã•ã›ã¦ã„ã¾ã™ã€‚

    <img src="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png" width="30%">
    """)
    return


@app.cell
def _(pipeline):
    # model: "impira/layoutlm-document-qa" (128M params)
    # ref: https://huggingface.co/impira/layoutlm-document-qa

    pipe_dqa = pipeline(model="impira/layoutlm-document-qa")
    out_dqa1 = pipe_dqa(
        image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
        question="What is the invoice number?",
    )
    out_dqa1[0]["score"] = round(out_dqa1[0]["score"], 3)
    out_dqa1
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Using pipeline on large models with ğŸ¤— accelarate

    `device_map="auto"` ã‚’æŒ‡å®šã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ä¸Šã§é©åˆ‡ã«åˆ†é…ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€å˜ä¸€ã®ãƒ‡ãƒã‚¤ã‚¹ã§ã¯ãƒ¡ãƒ¢ãƒªã«ä¹—ã‚Šåˆ‡ã‚‰ãªã„å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    """)
    return


@app.cell
def _(pipeline, torch):
    # model: "facebook/opt-1.3b" (1.3B params, heavy)
    # ref: https://huggingface.co/facebook/opt-1.3b

    pipe_acc1 = pipeline(model="facebook/opt-1.3b", dtype=torch.bfloat16, device_map="auto")
    pipe_acc1("This is a cool example!", do_sample=True, top_p=0.95)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ã•ã‚‰ã«ã€`bitsandbytes` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ä¸Šã€`load_in_8bit=True` ã‚’æŒ‡å®šã™ã‚Œã°ã€ãƒ¢ãƒ‡ãƒ«ã‚’ 8 bit ã§é‡å­åŒ–ã—ã¦èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
    ãŸã ã—ã€`bitsandbytes` ã¯ç¾çŠ¶ linux ã¨ windows ã—ã‹ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚
    """)
    return


@app.cell
def _():
    # pipe_8bit1 = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
    # pipe_8bit1("This is a cool example!", do_sample=True, top_p=0.95)
    return


@app.cell
def _():
    import marimo as mo
    return (mo,)


if __name__ == "__main__":
    app.run()
